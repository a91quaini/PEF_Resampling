---
title: "Bootstapping Lasso Estimators"
author: "Alberto Quaini"
date: "June 2017"
output:
  beamer_presentation
header-includes:
   - \usepackage{mathrsfs}
   - \usepackage{bbm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Presentation


* _Presentation of:_  
"Bootstrapping Lasso Estimator" -- A. Chatterjee, S. N. Lahiri [2011], JASA.  

* _for PEF UNISG course:_  
"Resampling methods and forecasting" -- L. Camponovo

* _Additional literature:_   
1  
2


## Outline

1. Introduction
2. The Modified Bootstrap method
    + Background and motivation
    + A Modified Bootstrap method
3. Bootstrapping the Lasso estimator
    + Consistency and the distributional approximation
    + Bootstrap bias and variance estimation
4. Bootstrapping the Adaptive Lasso estimator
    + A residual Bootstrap method for the Adaptive Lasso estimator
    + Main results
5. Data-based choice of the regularization parameter for the Lasso estimator
    + The optimal regularization parameter
    + Data-based selection of the optimal regularization parameter
    + Jackknife-After-Bootstrap based choice of the regularization parameter
6. Numerical results


## Introduction

Linear regression model with iid errors:
\begin{equation}
y_i = x_i^T \beta + \epsilon_i, \qquad i=1,...,n
\end{equation}

Lasso estimator:
\begin{equation}
\hat{\beta}_n = argmin_{u \in R^p} \sum_{i=1}^n (y_i - x_i^T u)^2 
+ \lambda_n \sum_{j=1}^p |u_j|
\end{equation}
  + estimation and variable selection method (Tibshirani [1996])
  + computationally feasible (Friedman et Al. [2007])
  + model consistency (Wainwright [2006], Zhao and Yu [2006] and Zou [2006])
  + estimation consistency (Knight and Fu [2000])

## Introduction

Problems:  

1. Consistency
    + Knight and Fu [2000] show that the limiting distribution of the Lasso estimator is complicated
    + in practice alternative approximations are needed to carry on inference for the Lasso
    + The two autors consider the residual-based Bootstrap method
    + Chatterjee and Lahiri [2010] show that the Bootstrapped Lasso estimator is inconsistent whenever at least one component of the parameter vector is zero

2. Confidence intervals and testing
    + proposals of Tibshirani [1996] and Osborne et Al. [2000] have the drawback of considering the Lasso an approximately linear transformation
    + proposals of Tibshirani [1996], Fan and Li [2001] and Fan and Peng [2004] only provide CI for underlying non-zero parameters


## Introduction

Results and proposals in Chatterjee and Lahiri [2011]:  

1. Consistency  
    + construct a suitable modification to the residual-based Bootstrap 
    + show consistency under mild regularity conditions even when some of the underlying parameters are zero

2. Confidence interval and testing
    + the modified Bootstrap method provides consistent estimate of the variance of the Lasso estimator for both zero and non-zero parameter components


## Introduction

3. choice of the regularization parameter $\lambda_n$
    + accuracy of the lasso critically depends on the regularization parameter
    + the modified Bootstrap is consistent for the MSE of the Lasso
    + the modified Bootstrap estimator of the MSE can be used for the choice of $\lambda_n$
    
4. Adaptive Lasso estimator (Zou [2006])
    + adaptive weights are used for penalizing different coefficients in the $L_1$ penalty
    + it enjoys the oracle property, i.e. performs as well as if the true underlying model were given in advance
    + the authors show that the simple residual Bootstrap can consistently estimate the distribution of the adaptive Lasso estimator
    

## The Modified Bootstrap method

#### background and motivation

The residual Bootstrap method (standard in linear regression setting with nonrandom $x_i$, see Efron [1979], Freedman [1981]) proceeds as follows in the context of the Lasso (Knight and Fu [2000]):

1. Consider the set of centered residuals $E = \{ e_i = \tilde{e}_i - \bar{e}, \text{for } i=1,...,n \}$, where $\bar{e} = n^{-1} \sum_i \tilde{e}_i$ and $\tilde{e}_i$'s are the residuals of the Lasso fit on the original sample.
2. Construct $B$ bootstrap samples of size $n$ selecting with replacement form $E$: $E_b^* = \{e_{i,b}^*: i=1,...,n \}$ and compute $y_{i,b}^* = x_i^T \hat{\beta}_n + e_{i,b}^*$, for $i=1,...,n, \text{and } b=1,...,B$, where $\hat{\beta}_n$ is the Lasso estimator for the original sample.


## The Modified Bootstrap method

3. Compute the bootstrap version of $T_n = n^{1/2}(\hat{\beta}_n - \beta)$, i.e. $T_n^* = n^{1/2}(\hat{\beta}_{n,b}^* - \hat{\beta}_n)$, where $\hat{\beta}_{n,b}^*$ is the Lasso estimator for bootstrap sample $b$. 
4. The residual Bootstrap estimator of the distribution $G_n$ of $T_n$ is $\hat{G}_n(B) = P_*(T_n^* \in B)$, where $B \in \mathscr{B}(R^p)$ and $P_*$ is the probability of $T_n^*$ given errors $\epsilon_i$'s.

Chatterjee and Lahiri [2010] show that:
\begin{itemize}
\item the estimators of the zero parameters fail to capture the target sign value, which is zero
\item because of that, $\hat{G}n$, instead of converging to the deterministic limit of $Gn$ converges weakly to a random probability measure
\item i.e. it fails to provide a valid approximation to $G_n$
\end{itemize}


## The Modified Bootstrap method

#### A Modified Bootstrap method

Objective: capture the signs of the parameters, expecially the zero components, with probability tending to 1, as the sample size $n$ goes to infinity.

Idea: force components of the Lasso estimator $\hat{\beta}_n$ to be exactly zero whenever they are close to zero using the fact that the Lasso estimator is root-n consistent.

To this end:

1. Form a sequence $\{a_n\}$ of real numbers such that $a_n + (n^{-1/2} log(n)) a_n^{-1} \rightarrow 0$ asymptotically.
2. Threshold the components of the Lasso estimator $\beta_n$ at $a_n$, and define the modified Lasso estimator 
\begin{equation}
\tilde{\beta}_{n,j} = \beta_{n,j} \mathbbm{1}(\beta_{n,j} \geq a_n), \text{for } j=1,...,p.
\end{equation}


## The Modified Bootstrap method

Note that with probability tending to 1 (as $n \rightarrow \infty$):

\begin{itemize}
\item $|\hat{\beta}_{n,j}| = |\beta_j| + O(n^{-1/2}) > |\beta_j|/2 \geq a_n$, for a nonzero component $\beta_j$
\item $|\hat{\beta}_{n,j}| = |\beta_j| + O(n^{-1/2}) = O(n^{-1/2}) \in [-a_n, a_n]$, for a zero component $\beta_j$
\end{itemize}

Then proceed as before:

3. Consider the set of centered residuals $R = \{ r_i = \tilde{r}_i - \bar{r}, \text{for } i=1,...,n \}$, where $\bar{r} = n^{-1} \sum_i \tilde{r}_i$ and $\tilde{r}_i$'s are the residuals of the modified Lasso fit on the original sample.
4. Construct $B$ bootstrap samples of size $n$ selecting with replacement form $R$: $R_b^{**} = \{r_{i,b}^{**}: i=1,...,n \}$ and compute $y_{i,b}^{**} = x_i^T \tilde{\beta}_n + r_{i,b}^{**}$, for $i=1,...,n, \text{and } b=1,...,B$, where $\tilde{\beta}_n$ is the modified Lasso estimator for the original sample.


## The Modified Bootstrap method

5. Compute the bootstrap version of $T_n = n^{1/2}(\hat{\beta}_n - \beta)$, i.e. $T_n^{**} = n^{1/2}(\hat{\beta}_{n,b}^{**} - \tilde{\beta}_n)$, where $\hat{\beta}_{n,b}^{**}$ is the Lasso estimator for bootstrap sample $b$. 
6. The residual Bootstrap estimator of the distribution $G_n$ of $T_n$ is $\tilde{G}_n(B) = P_{**}(T_n^{**} \in B)$, where $B \in \mathscr{B}(R^p)$ and $P_{**}$ is the probability of $T_n^{**}$ given errors $\epsilon_i$'s.

Remarks:

\begin{itemize}
\item Centering the residuals ensures the Bootstrap analogue of the condition $E[e_i]=0$
\item A rescaling factor $(1 - p/n) - 1/2$ is sometimes used in the construction of the residuals (see Efron [1982]) to improve finite sample accuracy
\item It is possible to replace $\hat{\beta}_n$ by any other $\sqrt{n}$-consistent estimator of $\beta$, e.g. least squares
\end{itemize}


## Bootstrapping the Lasso estimator

#### Consistency and the distributional approximation

*Theorem 1: Consistency of Modified Bootstrap*

Assume: 

\begin{itemize}
\item (C1) $n^{-1} \sum_i x_ix_i^T \rightarrow C,$ p.d. matrix. Furthemore $n^{-1} \sum_i \|x_i\|^3 \rightarrow O(1)$.
\item (C2) $\lambda_n n^{-1/2} \rightarrow \lambda_0 \geq 0$.
\item (C3) errors $\epsilon_i$'s are iid with $E[\epsilon_i]=0$ and $VAR[\epsilon_i]=\sigma^2<\infty$.
\end{itemize}

Then: $$\mathscr{P}(\tilde{G}_n, G_n) \rightarrow 0, \text{as } n \rightarrow \infty, \text{with probability } 1,$$

where $\mathscr{P}(\cdot,\cdot)$ denotes the Prohorov probability metric.


## Bootstrapping the Lasso estimator

Remarks:

\begin{itemize}
\item Chatterjee and Lahiri [2010] shows that under the same set of regularity assumptions, if $\beta$ has at least one zero component and if $\hat{G}_n$ is the residual bootstrap estimate of $G_n$, then
$$\mathscr{P}(\hat{G}_n, G_n) \not\to 0, \text{in probability, as } n \rightarrow \infty$$
\item Theorem 1 states strong consistency of the modified Bootstrap distribution estimator
\item From Theorem 1 it follows that the modified bootstrap method can be used to approximate the distribution of the Lasso estimator $T_n$ for any $\beta \in R^p$. Hence, it can be used to construct valid large sample confidence set estimators of $\beta$
\end{itemize}


## Bootstrapping the Lasso estimator

Definitions:

\begin{itemize}
\item let $t(\alpha)$ denote the $\alpha \in (0,1)$ quantile of $\|T_{\infty}\|$, where $T_{\infty}$ denotes the limiting random vector such that $T_n \rightarrow T_{\infty}$ and has distribution $G_{\infty}$.
\item let $\hat{t}_n(\alpha)$ denote the $\alpha \in (0,1)$ quantile of the bootstrap distribution of $\|T_n^{**}\|$. Then the set $$I_{n,\alpha} \equiv \{ t \in R^p: \|t-\hat{\beta}_n \| \leq n^{-1/2} \hat{t}_n(\alpha) \}.$$
\end{itemize}


## Bootstrapping the Lasso estimator

*Corollary 1: Modified Bootstrap Confidence Interval*

Assume (C1), (C2) and (C3) hold. Then:

\begin{enumerate}[i]
\item if $\alpha \in (0,1)$ is such that $P(\|T_{\infty} \| \leq t(\alpha) + \eta) > \alpha, \forall \eta>0$, then for all $\beta \in R^p$: 
\begin{equation}
\label{CI_consistency}
P(\beta \in I_{n,\alpha}) \rightarrow \alpha, \qquad \text{as } n \rightarrow \infty
\end{equation}
\item if there is at least one nonzero component of $\beta$, then (\ref{CI_consistency}) holds for all $\alpha \in (0,1)$.
\end{enumerate}

\begin{itemize}
\item Corollary 1 justifies the use of the modified Bootstrap method to construct valid large sample confidence regions for $\beta$
\item Corollary 1 can also be used to test the null hypothesis $H_0: \beta_j=0$ for all $j\in J$ for a given $J\subset \{1,...,p \}$
\end{itemize}


## Bootstrapping the Lasso estimator

Remarks:

\begin{itemize}
\item Leeb and P\"{o}tscher [2006, 2008] and P\"{o}tscher and Schneider [2009] show that it is impossible to consistently estimate the distribution function of the Lasso estimator in a uniform sense
\item Problems arise expecially when some underlying nonzero parameters get close to zero as $n$ gets large
\item Theorem 1 provides a method to obtain a consistent estimator in case the underlying parameters are fixed
\item Andrews and Guggenberger [2009] show that uniform consistency is not necessary for producing uniformly valid confidence intervals
\item Corollary 1 asserts that the modified Bootstrap method can control the asymptotic size of confidence intervals, however it is not clear if the latters are uniformly valid in the parameter values
\end{itemize}