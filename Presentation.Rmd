---
title: "Bootstapping Lasso Estimators"
author: "Alberto Quaini"
date: "June 2017"
output:
  beamer_presentation
header-includes:
   - \usepackage{mathrsfs}
   - \usepackage{bbm}
   - \usepackage{amsmath}
   - \usepackage{graphicx}
   - \graphicspath{{Images/}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Presentation


_Presentation of:_  

* "Bootstrapping Lasso Estimator" -- A. Chatterjee, S. N. Lahiri [2011], JASA.  

_for PEF UNISG course:_  

* "Resampling methods and forecasting" -- L. Camponovo

_Complementary literature:_ 

* "Asymptotics properties of the residual Bootstrap
for Lasso estimators" -- A. Chattarjee, S. N. Lahiri [2010]

* "Bootstrap-based penalty choice for the Lasso, achieving oracle performance"
P. Hall, E. R. Lee and B. U. Park [2009]


## Outline

1. Introduction
2. The Modified Bootstrap method
    + Background and motivation
    + A Modified Bootstrap method
3. Bootstrapping the Lasso estimator
    + Consistency and the distributional approximation
    + Bootstrap bias and variance estimation
4. Bootstrapping the Adaptive Lasso estimator
    + A residual Bootstrap method for the Adaptive Lasso estimator
    + Main results

    
## Outline

5. Data-based choice of the regularization parameter
    + The optimal regularization parameter
    + Data-based selection of the optimal regularization parameter
    + Jackknife-After-Bootstrap based choice of the regularization parameter
6. Numerical results
    + Choice of optimal penalization and thresholding parameters
    + Coverage accuracy of confidence regions
    + Variance estimation


## 1. Introduction

Linear regression model with iid errors:
\begin{equation}
y_i = x_i^T \beta + \epsilon_i, \qquad i=1,...,n
\end{equation}

\textbf{Lasso} estimator:
\begin{equation}
\hat{\beta}_n = argmin_{u \in R^p} \sum_{i=1}^n (y_i - x_i^T u)^2 
+ \lambda_n \sum_{j=1}^p |u_j|
\end{equation}
  + estimation and variable selection method (Tibshirani [1996])
  + computationally feasible (Friedman et Al. [2007])
  + model consistency (Wainwright [2006], Zhao and Yu [2006] and Zou [2006])
  + estimation consistency (Knight and Fu [2000])

## 1. Introduction

Problems:  

1. Consistency
    + Knight and Fu [2000] show that the limiting distribution of the Lasso estimator is complex in the situation of sparse underlying parameter vector
    + in practice alternative approximations are needed to carry on inference for the Lasso
    + Chatterjee and Lahiri [2010] show that the Bootstrapped Lasso estimator based on the residual Bootstrap method is inconsistent whenever at least one component of the parameter vector is zero

2. Confidence intervals and testing
    + proposals of Tibshirani [1996] and Osborne et Al. [2000] have the drawback of considering the Lasso an approximately linear transformation
    + proposals of Tibshirani [1996], Fan and Li [2001] and Fan and Peng [2004] only provide CI for underlying non-zero parameters


## 1. Introduction

Results and proposals in Chatterjee and Lahiri [2011]:  

1. Consistency  
    + construct a suitable modification to the residual-based Bootstrap 
    + show consistency under mild regularity conditions even when some of the underlying parameters are zero

2. Confidence interval and testing
    + the modified Bootstrap method provides consistent estimate of the variance of the Lasso estimator for both zero and non-zero parameter components


## 1. Introduction

3. choice of the regularization parameter $\lambda_n$
    + accuracy of the Lasso critically depends on the regularization parameter
    + the modified Bootstrap is consistent for the MSE of the Lasso, hence it can be used for selecting $\lambda_n$
    
4. Adaptive Lasso estimator (Zou [2006])
    + adaptive weights are used for penalizing differently the coefficients in the $L_1$ penalty
    + it enjoys the oracle property, i.e. performs as well as if the true underlying model were given in advance
    + Chatterjee and Lahiri [2011] show that the simple residual Bootstrap can consistently estimate the distribution of the adaptive Lasso estimator
    

## 2. The Modified Bootstrap method

#### background and motivation

The \textbf{residual Bootstrap} method (standard in linear regression setting with nonrandom $x_i$, see Efron [1979], Freedman [1981]) proceeds as follows in the context of the Lasso (Knight and Fu [2000]):

1. Consider the set of centered residuals $E = \{ e_i = \tilde{e}_i - \bar{e}, \text{for } i=1,...,n \}$, where $\bar{e} = n^{-1} \sum_i \tilde{e}_i$ and $\tilde{e}_i$'s are the residuals of the Lasso fit on the original sample.
2. Construct $B$ bootstrap samples of size $n$ selecting with replacement form $E$: $E_b^* = \{e_{i,b}^*: i=1,...,n \}$ and compute $y_{i,b}^* = x_i^T \hat{\beta}_n + e_{i,b}^*$, for $i=1,...,n, \text{and } b=1,...,B$, where $\hat{\beta}_n$ is the Lasso estimator for the original sample.


## 2. The Modified Bootstrap method

3. Compute the bootstrap version of $T_n = n^{1/2}(\hat{\beta}_n - \beta)$, i.e. $T_n^* = n^{1/2}(\hat{\beta}_{n,b}^* - \hat{\beta}_n)$, where $\hat{\beta}_{n,b}^*$ is the Lasso estimator for bootstrap sample $b$. 
4. The residual Bootstrap estimator of the distribution $G_n$ of $T_n$ is $\hat{G}_n(B) = P_*(T_n^* \in B)$, where $B \in \mathscr{B}(R^p)$ and $P_*$ is the probability of $T_n^*$ given errors $\epsilon_i$'s.

Chatterjee and Lahiri [2010] show that:
\begin{itemize}
\item the estimators of the zero parameters fail to capture the target sign value, which is zero
\item because of that, $\hat{G}n$, instead of converging to the deterministic limit of $Gn$ converges weakly to a random probability measure
\item i.e. it fails to provide a valid approximation to $G_n$
\end{itemize}


## 2. The Modified Bootstrap method

#### A Modified Bootstrap method

Objective: capture the signs of the parameters, expecially the zero components, with probability tending to 1, as the sample size $n$ goes to infinity.

Idea: force components of the Lasso estimator $\hat{\beta}_n$ to be exactly zero whenever they are close to zero using the fact that the Lasso estimator is root-n consistent.

To this end:

1. Form a sequence $\{a_n\}$ of real numbers such that $a_n + (n^{-1/2} log(n)) a_n^{-1} \rightarrow 0$ asymptotically.
2. Threshold the components of the Lasso estimator $\beta_n$ at $a_n$, and define the modified Lasso estimator 
\begin{equation}
\label{ModLasso}
\tilde{\beta}_{n,j} = \beta_{n,j} \mathbbm{1}(\beta_{n,j} \geq a_n), \text{for } j=1,...,p.
\end{equation}


## 2. The Modified Bootstrap method

Note that with probability tending to 1 (as $n \rightarrow \infty$):

\begin{itemize}
\item $|\hat{\beta}_{n,j}| = |\beta_j| + O(n^{-1/2}) > |\beta_j|/2 \geq a_n$, for a nonzero component $\beta_j$
\item $|\hat{\beta}_{n,j}| = |\beta_j| + O(n^{-1/2}) = O(n^{-1/2}) \in [-a_n, a_n]$, for a zero component $\beta_j$
\end{itemize}

Then proceed as before:

3. Consider the set of centered residuals $R = \{ r_i = \tilde{r}_i - \bar{r}, \text{for } i=1,...,n \}$, where $\bar{r} = n^{-1} \sum_i \tilde{r}_i$ and $\tilde{r}_i$'s are the residuals of the modified Lasso fit on the original sample.
4. Construct $B$ bootstrap samples of size $n$ selecting with replacement form $R$: $R_b^{**} = \{r_{i,b}^{**}: i=1,...,n \}$ and compute $y_{i,b}^{**} = x_i^T \tilde{\beta}_n + r_{i,b}^{**}$, for $i=1,...,n, \text{and } b=1,...,B$, where $\tilde{\beta}_n$ is the modified Lasso estimator for the original sample.


## 2. The Modified Bootstrap method

5. Compute the bootstrap version of $T_n = n^{1/2}(\hat{\beta}_n - \beta)$, i.e. $T_n^{**} = n^{1/2}(\hat{\beta}_{n,b}^{**} - \tilde{\beta}_n)$, where $\hat{\beta}_{n,b}^{**}$ is the Lasso estimator (not the modified one) for bootstrap sample $b$. 
6. The residual Bootstrap estimator of the distribution $G_n$ of $T_n$ is $\tilde{G}_n(B) = P_{**}(T_n^{**} \in B)$, where $B \in \mathscr{B}(R^p)$ and $P_{**}$ is the probability of $T_n^{**}$ given errors $\epsilon_i$'s.

Remarks:

\begin{itemize}
\item Centering the residuals ensures the Bootstrap analogue of the condition $E[e_i]=0$
\item A rescaling factor $(1 - p/n) - 1/2$ is sometimes used in the construction of the residuals (see Efron [1982]) to improve finite sample accuracy
\item It is possible to replace $\hat{\beta}_n$ by any other $\sqrt{n}$-consistent estimator of $\beta$, e.g. least squares
\end{itemize}


## 3. Bootstrapping the Lasso estimator

#### Consistency and the distributional approximation

\textbf{Theorem 1: Consistency of Modified Bootstrap}

Assume: 

\begin{itemize}
\item (C1) $n^{-1} \sum_i x_ix_i^T \rightarrow C,$ p.d. matrix. Furthemore $n^{-1} \sum_i \|x_i\|^3 \rightarrow O(1)$.
\item (C2) $\lambda_n n^{-1/2} \rightarrow \lambda_0 \geq 0$.
\item (C3) errors $\epsilon_i$'s are iid with $E[\epsilon_i]=0$ and $VAR[\epsilon_i]=\sigma^2<\infty$.
\end{itemize}

Then: $$\mathscr{P}(\tilde{G}_n, G_n) \rightarrow 0, \text{as } n \rightarrow \infty, \text{with probability } 1,$$

where $\mathscr{P}(\cdot,\cdot)$ denotes the Prohorov probability metric.


## 3. Bootstrapping the Lasso estimator

Remarks:

\begin{itemize}
\item Chatterjee and Lahiri [2010] shows that under the same set of regularity assumptions, if $\beta$ has at least one zero component and if $\hat{G}_n$ is the residual bootstrap estimate of $G_n$, then
$$\mathscr{P}(\hat{G}_n, G_n) \not\to 0, \text{in probability, as } n \rightarrow \infty$$
\item Theorem 1 states strong consistency of the modified Bootstrap distribution estimator
\item From Theorem 1 it follows that the modified bootstrap method can be used to approximate the distribution of the Lasso estimator $T_n$ for any $\beta \in R^p$. Hence, it can be used to construct valid large sample confidence set estimators of $\beta$
\end{itemize}


## 3. Bootstrapping the Lasso estimator

Definitions:

\begin{itemize}
\item let $t(\alpha)$ denote the $\alpha \in (0,1)$ quantile of $\|T_{\infty}\|$, where $T_{\infty}$ denotes the limiting random vector such that $T_n \rightarrow T_{\infty}$ and has distribution $G_{\infty}$.
\item let $\hat{t}_n(\alpha)$ denote the $\alpha \in (0,1)$ quantile of the bootstrap distribution of $\|T_n^{**}\|$. Then the set $$I_{n,\alpha} \equiv \{ t \in R^p: \|t-\hat{\beta}_n \| \leq n^{-1/2} \hat{t}_n(\alpha) \}.$$
\end{itemize}


## 3. Bootstrapping the Lasso estimator

\textbf{Corollary 1: Modified Bootstrap Confidence Interval}

Assume (C1), (C2) and (C3) hold. Then:

\begin{enumerate}[i]
\item if $\alpha \in (0,1)$ is such that $P(\|T_{\infty} \| \leq t(\alpha) + \eta) > \alpha, \forall \eta>0$, then for all $\beta \in R^p$: 
\begin{equation}
\label{CI_consistency}
P(\beta \in I_{n,\alpha}) \rightarrow \alpha, \qquad \text{as } n \rightarrow \infty
\end{equation}
\item if there is at least one nonzero component of $\beta$, then (\ref{CI_consistency}) holds for all $\alpha \in (0,1)$.
\end{enumerate}

\begin{itemize}
\item Corollary 1 justifies the use of the modified Bootstrap method to construct valid large sample confidence regions for $\beta$
\item Corollary 1 can also be used to test the null hypothesis $H_0: \beta_j=0$ for all $j\in J$ for a given $J\subset \{1,...,p \}$
\end{itemize}


## 3. Bootstrapping the Lasso estimator

Remarks:

\begin{itemize}
\item Leeb and P\"{o}tscher [2006, 2008] and P\"{o}tscher and Schneider [2009] show that it is impossible to consistently estimate the distribution function of the Lasso estimator in a uniform sense
\item Problems arise expecially when some underlying nonzero parameters get close to zero as $n$ gets large
\item Theorem 1 provides a method to obtain a consistent estimator in case the underlying parameters are fixed
\item Andrews and Guggenberger [2009] show that uniform consistency is not necessary for producing uniformly valid confidence intervals
\item Corollary 1 asserts that the modified Bootstrap method can control the asymptotic size of confidence intervals, however it is not clear if the latters are uniformly valid in the parameter values
\end{itemize}


## 3. Bootstrapping the Lasso estimator

#### Bootstrap bias and variance estimation

\textbf{Theorem 2: Bias and Variance Consistency}

Assume (C1), (C2) and (C3) hold. Then with probability $1$:
\begin{align}
E_*[T_n^{**}] \rightarrow E[T_{\infty}] \qquad \text{and} \\
(VAR_*[T_n^{**}])_{p \times p} \rightarrow (VAR_*[T_{\infty}])_{p \times p}
\end{align}

\begin{itemize}
\item for $\lambda_0 \neq 0$ in assumption (C2), $T_n$ may be asymptotically biased and standard MSE estimation methods are unreliable
\item The modified Bootstrap method produces strongly consistent estimators of the asymptotic bias and variance of $T_n$
\item hence, Theorem 2 allows to estimate the MSE of a Lasso estimate and quantify the associated uncertainty for all values of $\beta$
\end{itemize}


## 3. Bootsrapping the Adaptive Lasso estimator

The \textbf{adaptive Lasso} estimator (Zou [2006]):
\begin{equation}
\breve{\beta}_n = argmin_{u \in R^p} \sum_{i=1}^n (y_i - x_i^Tu)^2 + \lambda_n \sum_{j=1}^p \frac{|u_j|}{|\bar{\beta}_{j,n}|^{\gamma}},
\end{equation}

where $\bar{\beta}_n$ denote an initial consistent estimator of $\beta$ (e.g. least squares), $\lambda_n \geq 0$ is the penalty and $\gamma >0$.

\textbf{Oracle property} (Zou [2006]):
\begin{align}
P(B_n = A) \rightarrow 1, \quad \text{as } n \rightarrow 1 \\
\sqrt{n}(\breve{\beta}_n^{nz} - \beta^{nz}) \rightarrow N(0, \sigma^2 C_{nz})
\end{align}

where $A = \{ j:\beta_j = 0 \}$, $B_n = \{j: \breve{\beta}_{j,n} = 0 \}$ and $\sigma^2 C_{nz}$ is the var-cov matrix between nonzero estimated and underlying parameters 


## 3. Bootsrapping the Adaptive Lasso estimator

#### A residual Bootstrap method for Adaptive Lasso

The algorithm is similar to the residual Bootstrap described earlier with few adjustments:
\begin{itemize}
\item $E$ becomes the set of centered residuals of the adaptive Lasso fit on the original sample
\item for each bootstrap sample, the adaptive Lasso estimator becomes:
\begin{equation}
\breve{\beta}_n^+ = argmin_{u \in R^p} \sum_{i=1}^n (y_i^+ - x_i^Tu)^2 + \lambda_n \sum_{j=1}^p \frac{|u_j|}{|\bar{\beta}_{j,n}^+|^{\gamma}},
\end{equation}
where $y_i^+ = x_i^T\breve{\beta}_n + \epsilon_i^+, i=1,...,n$, $\epsilon_i^+$'s are bootstrapped from $E$ and $\bar{\beta}_n^+$ is defined by replacing $y_i$'s with $y_i^+$'s in the definition of $\bar{\beta}_n$
\end{itemize}


## 4. Bootsrapping the Adaptive Lasso estimator

Remarks:
\begin{itemize}
\item the penalty of the adaptive Lasso incorporates a built-in soft-thresholding for the zero parameters
\item Hence, the Bootstrap procedure just described does not need an initial truncation as it is the case for the Lasso
\end{itemize}

Denote:
\begin{itemize}
\item $\breve{T}_n \equiv \sqrt{n}(\breve{\beta}_n - \beta)$ with distribution $H_n$, where $H_n(x)= P(\breve{T}_n \leq x), x \in R$
\item $\breve{T}_n^+ \equiv \sqrt{n}(\breve{\beta}_n^+ - \breve{\beta}_n)$ with distribution $H_n^+$ conditional on the $\epsilon_i$'s
\end{itemize}


## 4. Bootsrapping the Adaptive Lasso estimator

\textbf{Theorem 3:}

Assume (C1), and (C3) hold and suppose
\begin{equation}
\frac{\lambda_n}{\sqrt{n}} \rightarrow 0 \quad \text{and} \quad \lambda_n n^{(\gamma - 1)/2} \rightarrow \infty.
\end{equation}
Then,
\begin{equation}
\label{adaLasso_CI}
\mathscr{P}(\hat{H}_n, H_n) \overset{P}\rightarrow 0, \quad \text{as } n \rightarrow \infty,
\end{equation}

Remarks:
\begin{itemize}
\item (\ref{adaLasso_CI}) can be used to construct valid confidence intervals for $\beta$
\item a corollary of the form of corollary 1 can be formulated for the adaptive Lasso residual bootstrap method
\end{itemize}


## 4. Bootsrapping the Adaptive Lasso estimator

\begin{itemize}
\item estimation of the MSE of the adaptive Lasso estimator can be difficult
\item adaptive Lasso residual Bootstrap method provides a consistent estimator of the MSE matrix of the scaled adaptie Lasso estimator $\breve{\beta}_n$ given by $MSE[\breve{T}_n] \equiv n E[(\breve{\beta}_n - \beta)(\breve{\beta}_n - \beta)^T]$
\end{itemize}

\textbf{Corollary 3}

Assume (C1) and (C2) hold, Then:
\begin{equation}
MSE_*(\breve{T}_n^+) - MSE(\breve{T}_n) \overset{P}\rightarrow 0, \quad \text{as } n \rightarrow n.
\end{equation}


## 5. Data-based choice of the regularization parameter

#### The optimal regularization parameter

Remarks:
\begin{itemize}
\item it can be shown that the distribution of $T_n$ depends on $\lambda_n$ only through $\lambda_0$
\item note that $MSE(\hat{\beta}_n)$ can be expressed as $n^{-1}E\|T_n\|^2$ and that $n MSE(\hat{\beta}_n)$ converges to the $MSE$ of the limiting random variable $T_{\infty}$
\item The effect of the penalization by $\lambda_n$ on the overall accuracy of $\hat{\beta}_n$ is reflected by its MSE
\item for what comes next, consider the natural reparametrization $\lambda_n = \lambda_0 n^{1/2}, \lambda_0 \in [0, \infty)$
\end{itemize}


## 5. Data-based choice of the regularization parameter

Then define the \textbf{optimal penalization} parameter as
\begin{equation}
\lambda_0^{opt} \equiv argmin \phi(\lambda_0)
\end{equation}
where $\phi(\lambda_0) = E\|T_{\infty}\|^2$.

* Thus, choosing $\lambda_0 = \lambda_0^{opt}$ yelds a Lasso estimator that minimizes the $MSE$ in large samples.


## 5. Data-based choice of the regularization parameter

#### Data-based selection of the optimal regularization parameter

$\lambda_0$ can be estimated using the modified bootstrap:
\begin{itemize}
\item for any $\lambda_0 \in [0, \infty)$ and thresholding value $a \in (0, \infty)$ (as defined in (\ref{ModLasso})), rewrite $T_n^{**} \equiv T_n^{**}(\lambda_0,a)$
\item the modified Bootstrap estimator of $\phi(\lambda_0)$ is \begin{equation}
\hat{\phi}_n(\lambda_0,a) \equiv E_* \| T_n^{**}(\lambda_0,a) \|^2
\end{equation}
\item by Theorem 1, $\hat{\phi}_n(\lambda_0,a)$ is a strongly consistent estimator of $\phi(\lambda_0)$
\end{itemize}

Therefore, an accurate estimator in large sample of $\lambda_0^{opt}$ is the Bootstrap estimator:
\begin{equation}
\lambda_{0,n}^* = argmin_{\lambda_0,a} \hat{\phi}_n(\lambda_0,a)
\end{equation}


## 5. Data-based choice of the regularization parameter

#### Jackknife-After-Bootstrap based choice of the regularization parameter

The authors propose to estimate the thresholding parameter $a$ via Jackknife-After-Bootstrap (JAB, see Efron [1992]). The basic idea of the method is:
\begin{itemize}
\item compute replicates of the Bootstrap estimator $\hat{\phi}_n(\lambda_0,a)$ by means of delete-1 JAB over the Bootstrap samples $\{T_n^{**}(b:\lambda_0,a): b=1,...,B \}$
\item construct an estimate of the error of the Bootstrap estimator $\hat{\phi}_n(\lambda_0,a)$ based on the JAB replicates
\item select a thresholding value $a$ that minimize this estimate of error over all possible values of $\lambda_0$ and $a$
\end{itemize}


## 6. Numerical results

#### Choice of optimal penalization and thresholding parameters

For the numerical results regarding the choice of the regularization and thresholding parameters the authors consider:
\begin{itemize}
\item fixed-design matrix and errors generated by independent standard normal distribution with sample size $n=250$
\item $p=10$ overall regression coefficients which $6$ are nonzero organized in three Cases:
  \begin{enumerate}[i]
  \item large nonzero coefficients
  \item some small nonzero coefficients
  \item some nonzero coefficients shrinking as $n$ gets large
  \end{enumerate}
\end{itemize}


## 6. Numerical results

Figures (reference) show the behaviour of the naive residual Bootstrap (NB) and the modified Bootstrap (MB) estimates of the optimal $\lambda_0$ in Case (i) and (ii), respectively.

Note:
\begin{itemize}
\item the vertical solid line represents the true optimal $\lambda_0$
\item MB estimates are much better than the NB for Case (i)
\item in Case (ii) the NB seem to perform better than the MB
\item however if the performance of the MB is stratified for different values of $a$, in some cases it achieves much better results
\item the MB is very sensible to the choice of $a$, as it impacts the recovery of true nonzero parameters
\end{itemize}


## 6. Numerical results

#### Figure 1

\includegraphics[scale=0.35]{Case1}


## 6. Numerical results

#### Figure 2

\includegraphics[scale=0.35]{Case2}


## 6. Numerical results

Regarding the choice of the threshold parameter $a$, the authors consider a grid of six values over which the optimum values of $a$ are computed.

Numerical results are found in Table 1 in the paper. 

The main results are:
\begin{itemize}
\item the JAB estimates of $a$ are very good for Case (i) but deteriorate for the other two cases
\item the choice of the grid is very important
\item the JAB method is good in situations where $\beta$ has distinct large nonzero coefficients
\end{itemize}


## 6. Numerical results

#### Coverage accuracy of confidence regions

The authors compare the finite sample performance of the confidence regions for $\beta$ obtained by using the naive and the modified Bootstrap procedures.

Numerical results are found in Tables 2 -- 4 in the paper. 

The main findings are:
\begin{itemize}
\item both methods perform well in terms of achieving the desired nominal coverage rate for all three Cases
\item the inconsistency of the NB does not have an effect on the coverage accuracy of the confidence regions
\item however the inconsistency of the NB makes it an unreliable choice
\end{itemize}


## 6. Numerical results

#### Variance estimation

The authors then compare the NB, the MB, the m-out-of-n Bootstrap (MNB) (Hall, Lee and Park [2009]) and the sandwich formula (see Zou [2006] and Feng and Peng [2004]) for estimating the variance of the Lasso estimator.

Numerical results are reported in Table 5 and 6 in the paper.

The main findings are:
\begin{itemize}
\item NB and MB have very comparable results
\item MB has some advantages over the NB when the underlying value of the parameter is zero
\item MNB has very poor performances, which gradually improve as $m$ (the Bootstrap sample size) increases
\item the sandwich estimator has very good performances, but they can be tremendously impacted by extreme observations in the data
\end{itemize}